{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfef7bdf-5a27-4b2b-a5be-b677d0020647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: unknown command \"list-remote\" for \"ollama\"\n"
     ]
    }
   ],
   "source": [
    "!ollama list-remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7895b8-5533-49b0-9913-e67fbd1c53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OLLAMA_DEVICE\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b29976-8538-4cf8-8ff8-73073547418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comparison saved: results/comparison_Cologne.bpmn_vs_Frankfurt.bpmn.txt\n",
      "üìÑ Initial mapping CSV saved: results/initial_mapping_Cologne.bpmn_vs_Frankfurt.bpmn.csv\n",
      "‚úÖ Comparison saved: results/comparison_IIS_Erlangen.bpmn_vs_Fu_Berlin.bpmn.txt\n",
      "üìÑ Initial mapping CSV saved: results/initial_mapping_IIS_Erlangen.bpmn_vs_Fu_Berlin.bpmn.csv\n",
      "‚úÖ Comparison saved: results/comparison_Hohenheim.bpmn_vs_Potsdam.bpmn.txt\n",
      "üìÑ Initial mapping CSV saved: results/initial_mapping_Hohenheim.bpmn_vs_Potsdam.bpmn.csv\n",
      "‚úÖ Comparison saved: results/comparison_Muenster.bpmn_vs_Tu_Munich.bpmn.txt\n",
      "üìÑ Initial mapping CSV saved: results/initial_mapping_Muenster.bpmn_vs_Tu_Munich.bpmn.csv\n"
     ]
    }
   ],
   "source": [
    "#final chain-of-thought con csv mappings\n",
    "#final chain-of-thought\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Forza l'uso della CPU per evitare problemi di memoria\n",
    "# -----------------------------\n",
    "os.environ[\"OLLAMA_DEVICE\"] = \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Funzioni di supporto\n",
    "# -----------------------------\n",
    "def extract_task_names(file_path):\n",
    "    \"\"\"Estrae i nomi dei task da un file BPMN.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        ns = {'bpmn': 'http://www.omg.org/spec/BPMN/20100524/MODEL'}\n",
    "        task_names = []\n",
    "        for task_type in ['task', 'userTask', 'manualTask', 'serviceTask', 'scriptTask',\n",
    "                          'businessRuleTask', 'sendTask', 'receiveTask', 'callActivity']:\n",
    "            for task in root.findall(f\".//bpmn:{task_type}\", ns):\n",
    "                name = task.attrib.get('name')\n",
    "                if name:\n",
    "                    task_names.append(name)\n",
    "        return task_names\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Errore parsing BPMN file {file_path}: {e}\")\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File non trovato: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def extract_mappings_from_response(text):\n",
    "    \"\"\"\n",
    "    Estrae le mapping dalla risposta LLama usando regex robuste.\n",
    "    \"\"\"\n",
    "    pattern = r'\"(.+?)\"\\s*->\\s*\"(.+?)\"\\s*\\[(VB|MC|HR),\\s*similarity:\\s*([0-9.]+)\\]'\n",
    "    matches = re.findall(pattern, text)\n",
    "    mappings = []\n",
    "    for a, b, category, sim in matches:\n",
    "        mappings.append({\n",
    "            \"task_model_1\": a,\n",
    "            \"task_model_2\": b,\n",
    "            \"similarity\": sim,\n",
    "            \"category\": category,\n",
    "        })\n",
    "    return mappings\n",
    "\n",
    "def compare_bpmn_files(file1_path, file2_path, output_dir=\"results\"):\n",
    "    \"\"\"Confronta due modelli BPMN usando Llama3 su CPU e salva il risultato su file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    name_1 = os.path.basename(file1_path)\n",
    "    name_2 = os.path.basename(file2_path)\n",
    "\n",
    "    tasks1 = extract_task_names(file1_path)\n",
    "    tasks2 = extract_task_names(file2_path)\n",
    "\n",
    "    if not tasks1 or not tasks2:\n",
    "        print(f\"Skipping comparison for {name_1} vs {name_2} due to missing tasks.\")\n",
    "        return\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "# Context and Role Specification \n",
    "You are a process analysis expert specialized in BPMN comparison, process similarity, and model alignment.\n",
    "\n",
    "# Scope and Task Definition \n",
    "Compare two BPMN models and identify correspondences (mappings) between their components.\n",
    "Then perform a structured self-critique and correction phase before producing final metrics.\n",
    "\n",
    "Input:\n",
    "BPMN Model 1: {name_1}\n",
    "{chr(10).join(tasks1)}\n",
    "\n",
    "BPMN Model 2: {name_2}\n",
    "{chr(10).join(tasks2)}\n",
    "\n",
    "#  Procedure Design and Output Structuring \n",
    "1) Identify all **1:1 correspondences** between elements of the two models.\n",
    "   (If 1:N or N:M exist, mention them briefly, but focus on 1:1.)\n",
    "2) For each mapping, compute a **similarity score (0‚Äì1)** using cosine similarity between vector embeddings of the element labels and context features.  \n",
    "   This score combines lexical, semantic, and functional resemblance.\n",
    "3) Classify each mapping as:\n",
    "   - VB (Verbatim): similarity > 0.90\n",
    "   - MC (Modified Copy): similarity 0.65‚Äì0.90\n",
    "   - HR (High Revision): similarity < 0.65\n",
    "4) Output mappings **grouped by category** (VB first, then MC, then HR) in this format:\n",
    "   \"Element A\" -> \"Element B\" [VB/MC/HR, similarity: x.xx]\n",
    "   Include a short justification (1‚Äì2 lines) for each mapping.\n",
    "5) After listing all mappings, perform a **self-critique and correction** phase:\n",
    "   - Identify potential errors or ambiguities (misclassifications, vague tasks, naming issues).\n",
    "   - Explain reasoning limits (missing context, unclear actor, inconsistent wording).\n",
    "   - Propose corrected or adjusted mappings when needed.\n",
    "   - Clearly separate **revised mappings** from initial ones.\n",
    "6) Conclude with **final metrics**:\n",
    "   - Total tasks in each model\n",
    "   - Count + % of VB, MC, HR\n",
    "   - Ambiguous / duplicated / missing tasks\n",
    "   - Global similarity score (weighted average of mapping similarities using cosine similarity)\n",
    "\n",
    "Expected output sections:\n",
    "A) Initial mappings (VB ‚Üí MC ‚Üí HR)\n",
    "B) Self-critique and revised mappings\n",
    "C) Final metrics and global similarity\n",
    "\n",
    "# Example Integration (Few-Shot In-Context Demonstration) \n",
    "## 1:1 Examples\n",
    "\"Record Payment\" -> \"Record Payment\" [VB, similarity: 1.00]  \n",
    "Explanation: identical label and meaning.\n",
    "\n",
    "\"Approve Order\" -> \"Approve Purchase Order\" [MC, similarity: 0.83]  \n",
    "Explanation: same intent, minor lexical addition.\n",
    "\n",
    "\"Create Contract\" -> \"Draft Agreement\" [HR, similarity: 0.47]  \n",
    "Explanation: significantly revised meaning and terminology.\n",
    "\n",
    "## 1:N Example\n",
    "\"Send Invoice\" -> [\"Generate Invoice\", \"Email Invoice\"] [similarity: 0.58]  \n",
    "Explanation: original task split into multiple steps; functionally equivalent but more granular.\n",
    "\n",
    "# === Step 5: Verification, Termination, and Iterative Refinement ===\n",
    "After producing the initial mappings:\n",
    "- Verify internal consistency across VB, MC, HR classifications.\n",
    "- Re-evaluate any ambiguous or borderline mappings.\n",
    "- If errors or mismatches are found, iteratively refine similarity scores and category labels (based on cosine similarity).\n",
    "- Terminate once mappings, critique, and metrics are coherent and internally justified.\n",
    "\n",
    "# === Final Instruction ===\n",
    "Execute all steps in order: mappings by category ‚Üí self-critique and corrections ‚Üí metrics ‚Üí verification and refinement.\n",
    "Return the complete structured analysis in English.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(model='llama3:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "        content = response['message']['content']\n",
    "\n",
    "        # Salva il risultato su file\n",
    "        filename = f\"{output_dir}/comparison_{name_1}_vs_{name_2}.txt\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        print(f\"‚úÖ Comparison saved: {filename}\")\n",
    "\n",
    "        # === Salvataggio mapping in CSV ===\n",
    "        mappings = extract_mappings_from_response(content)\n",
    "        csv_name = f\"{output_dir}/initial_mapping_{name_1}_vs_{name_2}.csv\"\n",
    "\n",
    "        with open(csv_name, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"task_model_1\", \"task_model_2\", \"similarity\", \"category\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(mappings)\n",
    "\n",
    "        print(f\"üìÑ Initial mapping CSV saved: {csv_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la comparazione {name_1} vs {name_2}: {e}\")\n",
    "\n",
    "def batch_compare_bpmn_files(bpmn_files, batch_size=2):\n",
    "    \"\"\"Esegue il confronto di tutti i file BPMN nella lista, batch ridotti per CPU.\"\"\"\n",
    "    for i in range(0, len(bpmn_files), batch_size):\n",
    "        batch = bpmn_files[i:i+batch_size]\n",
    "        for file1, file2 in combinations(batch, 2):\n",
    "            compare_bpmn_files(file1, file2)\n",
    "\n",
    "# -----------------------------\n",
    "# Lista dei file BPMN\n",
    "# -----------------------------\n",
    "bpmn_files = [\n",
    "    \"Cologne.bpmn\",\n",
    "    \"Frankfurt.bpmn\",\n",
    "    \"IIS_Erlangen.bpmn\",\n",
    "    \"Fu_Berlin.bpmn\",\n",
    "    \"Hohenheim.bpmn\",\n",
    "    \"Potsdam.bpmn\",\n",
    "    \"Muenster.bpmn\",\n",
    "    \"Tu_Munich.bpmn\",\n",
    "    \"Wuerzburg.bpmn\"\n",
    "]\n",
    "\n",
    "# Avvia il batch (batch_size piccolo per evitare errori di memoria)\n",
    "batch_compare_bpmn_files(bpmn_files, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34fd7153-4edc-478c-a341-7637f7f30287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comparison saved: results/comparison_IIS_Erlangen - Copia (1).bpmn_vs_Fu_Berlin (2).bpmn.txt\n",
      "‚úÖ Comparison saved: results/comparison_Hohenheim - Copia (2).bpmn_vs_Potsdam - Copia (1).bpmn.txt\n",
      "‚úÖ Comparison saved: results/comparison_Muenster - Copia (1).bpmn_vs_Tu_Munich - Copia (1).bpmn.txt\n"
     ]
    }
   ],
   "source": [
    "#final error-guided prompt\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "\n",
    "# -----------------------------\n",
    "# Forza l'uso della CPU per evitare problemi di memoria\n",
    "# -----------------------------\n",
    "os.environ[\"OLLAMA_DEVICE\"] = \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Carica il CSV unificato\n",
    "# -----------------------------\n",
    "initial_mapping_csv_path = \"results/mappings_dataset.csv\"\n",
    "try:\n",
    "    with open(initial_mapping_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        initial_mapping_csv = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è CSV unificato non trovato: {initial_mapping_csv_path}\")\n",
    "    initial_mapping_csv = \"NO_MAPPING_CSV_FOUND\"\n",
    "\n",
    "# Placeholder per ground truth e refinement rules se non sono separati\n",
    "ground_truth_mappings = \"GROUND_TRUTH_NOT_PROVIDED\"\n",
    "refinement_rules = \"REFINEMENT_RULES_NOT_PROVIDED\"\n",
    "\n",
    "# -----------------------------\n",
    "# Funzioni di supporto\n",
    "# -----------------------------\n",
    "def extract_task_names(file_path):\n",
    "    \"\"\"Estrae i nomi dei task da un file BPMN.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        ns = {'bpmn': 'http://www.omg.org/spec/BPMN/20100524/MODEL'}\n",
    "        task_names = []\n",
    "        for task_type in ['task', 'userTask', 'manualTask', 'serviceTask', 'scriptTask',\n",
    "                          'businessRuleTask', 'sendTask', 'receiveTask', 'callActivity']:\n",
    "            for task in root.findall(f\".//bpmn:{task_type}\", ns):\n",
    "                name = task.attrib.get('name')\n",
    "                if name:\n",
    "                    task_names.append(name)\n",
    "        return task_names\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Errore parsing BPMN file {file_path}: {e}\")\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File non trovato: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def compare_bpmn_files(file1_path, file2_path, output_dir=\"results\"):\n",
    "    \"\"\"Confronta due modelli BPMN usando Llama3 su CPU e salva il risultato su file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    name_1 = os.path.basename(file1_path)\n",
    "    name_2 = os.path.basename(file2_path)\n",
    "\n",
    "    tasks1 = extract_task_names(file1_path)\n",
    "    tasks2 = extract_task_names(file2_path)\n",
    "\n",
    "    if not tasks1 or not tasks2:\n",
    "        print(f\"Skipping comparison for {name_1} vs {name_2} due to missing tasks.\")\n",
    "        return\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prompt aggiornato con CSV unificato\n",
    "    # -----------------------------\n",
    "    prompt = f\"\"\"\n",
    "# Context and Role\n",
    "You are a process alignment expert specialized in BPMN comparison, semantic similarity, and self-evaluation.\n",
    "\n",
    "# Input Data\n",
    "Initial Mapping CSV:\n",
    "{initial_mapping_csv}\n",
    "\n",
    "Ground Truth Mappings:\n",
    "{ground_truth_mappings}\n",
    "\n",
    "Refinement Rules from Training:\n",
    "{refinement_rules}\n",
    "\n",
    "# Scope and Task Definition\n",
    "1) Apply the refinement rules to guide detection of errors and corrections.\n",
    "2) Identify errors in the CSV:\n",
    "   - False positives (mapping present but not in ground truth)\n",
    "   - False negatives (mapping missing but present in ground truth)\n",
    "   - Category errors (VB vs MC vs HR)\n",
    "3) Propose corrected mappings based on the refinement rules.\n",
    "4) Compute evaluation metrics:\n",
    "   - Precision = TP / (TP + FP)\n",
    "   - Recall = TP / (TP + FN)\n",
    "   - F1 = harmonic mean of Precision and Recall\n",
    "\n",
    "# Procedure Design and Output Structuring\n",
    "A) SelfCritique (detected errors + revised mappings using refinement rules)\n",
    "B) Evaluation (metrics vs. ground truth)\n",
    "\n",
    "# Verification, Termination, and Iterative Refinement\n",
    "Terminate once mappings, critique, and metrics are coherent and internally justified.\n",
    "Return the output in English and structured JSON-like blocks.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(model='llama3:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "        content = response['message']['content']\n",
    "\n",
    "        # Salva il risultato su file\n",
    "        filename = f\"{output_dir}/comparison_{name_1}_vs_{name_2}.txt\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        print(f\"‚úÖ Comparison saved: {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la comparazione {name_1} vs {name_2}: {e}\")\n",
    "\n",
    "def batch_compare_bpmn_files(bpmn_files, batch_size=2):\n",
    "    \"\"\"Esegue il confronto di tutti i file BPMN nella lista, batch ridotti per CPU.\"\"\"\n",
    "    for i in range(0, len(bpmn_files), batch_size):\n",
    "        batch = bpmn_files[i:i+batch_size]\n",
    "        for file1, file2 in combinations(batch, 2):\n",
    "            compare_bpmn_files(file1, file2)\n",
    "\n",
    "# -----------------------------\n",
    "# Lista dei file BPMN\n",
    "# -----------------------------\n",
    "bpmn_files = [\n",
    "    \"IIS_Erlangen - Copia (1).bpmn\",\n",
    "    \"Fu_Berlin (2).bpmn\",\n",
    "    \"Hohenheim - Copia (2).bpmn\",\n",
    "    \"Potsdam - Copia (1).bpmn\",\n",
    "    \"Muenster - Copia (1).bpmn\",\n",
    "    \"Tu_Munich - Copia (1).bpmn\",\n",
    "    \"Wuerzburg (1).bpmn\"\n",
    "]\n",
    "\n",
    "# Avvia il batch (batch_size piccolo per evitare errori di memoria)\n",
    "batch_compare_bpmn_files(bpmn_files, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f755a-4e9d-4f52-92ef-52fa7ac20bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
